<div align="justify">

# Isolated Sign Language Recognition (ISLR)

## Technologies Used 
MediaPipe, Python, NumPy, TensorFlow (Fully Connected Neural Networks, RNN, LSTM, GRU)

## Project Overview:
The ISLR project aimed at developing a deep learning application to recognize American Sign Language (ASL) signs, offering a solution to the challenge of language acquisition for the deaf and hard-of-hearing community. Utilizing Google's MediaPipe for spatial data extraction, the project processed videos from 21 participants signing 250 unique words, creating a dataset that captures the nuances of ASL.

## Responsibilities:

Collaborated in designing and implementing data preprocessing pipelines to transform raw video data into structured formats suitable for deep learning models.
Contributed to feature engineering, extracting mean and standard deviation of landmark indices for a Fully Connected Neural Network (FCNN) and selecting specific landmark indices for sequence-to-sequence models.
Participated in the development and training of four different models: FCNN, RNN, LSTM, and GRU, to identify ASL signs from video data.
Engaged in iterative testing and optimization of models to improve accuracy, precision, recall, and F1 scores, applying advanced techniques such as GELU activation, dropout, batch normalization, layer normalization, and piecewise constant decay for learning rate adjustment.
Analyzed model performance, contributing to a comprehensive evaluation that compared the effectiveness of various architectures in sign language recognition.

## Achievements:

Achieved a breakthrough in sign language recognition with a GRU model, demonstrating the highest accuracy and performance metrics among the models tested.
Contributed to a significant project aiming to alleviate communication barriers for the deaf and hard-of-hearing community, with the potential to impact language acquisition and integration.
Gained extensive experience in applying deep learning techniques to a novel and socially impactful domain, enhancing skills in data preprocessing, model development, and performance analysis.

## Impact:

This project underscored the potential of deep learning in bridging communication gaps for individuals with hearing impairments. By achieving a 66% accuracy with the GRU model, it laid a foundation for future advancements in real-time sign language recognition, promising a more inclusive and accessible future for the deaf and hard-of-hearing community.

</div>
